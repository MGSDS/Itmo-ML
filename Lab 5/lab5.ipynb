{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UHX9p5jfTySS"
   },
   "source": [
    "## Задание 5.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0EnHNZtbXlH0"
   },
   "source": [
    "Набор данных тут: https://github.com/sismetanin/rureviews, также есть в папке [Data](https://drive.google.com/drive/folders/1YAMe7MiTxA-RSSd8Ex2p-L0Dspe6Gs4L). Те, кто предпочитает работать с английским языком, могут использовать набор данных `sms_spam`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.tokenize import word_tokenize as wt\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('women-clothing-accessories.3-class.balanced.csv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>качество плохое пошив ужасный (горловина напер...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Товар отдали другому человеку, я не получила п...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ужасная синтетика! Тонкая, ничего общего с пре...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>товар не пришел, продавец продлил защиту без м...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Кофточка голая синтетика, носить не возможно.</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  качество плохое пошив ужасный (горловина напер...  negative\n",
       "1  Товар отдали другому человеку, я не получила п...  negative\n",
       "2  Ужасная синтетика! Тонкая, ничего общего с пре...  negative\n",
       "3  товар не пришел, продавец продлил защиту без м...  negative\n",
       "4      Кофточка голая синтетика, носить не возможно.  negative"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>90000</td>\n",
       "      <td>90000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>87321</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Товар не пришёл</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>58</td>\n",
       "      <td>30000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  review sentiment\n",
       "count              90000     90000\n",
       "unique             87321         3\n",
       "top     Товар не пришёл   negative\n",
       "freq                  58     30000"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bJox-LoonoPx"
   },
   "source": [
    "Применим полученные навыки и решим задачу анализа тональности отзывов. \n",
    "\n",
    "Нужно повторить весь пайплайн от сырых текстов до получения обученной модели.\n",
    "\n",
    "Обязательные шаги предобработки:\n",
    "1. токенизация\n",
    "2. приведение к нижнему регистру\n",
    "3. удаление стоп-слов\n",
    "4. лемматизация\n",
    "5. векторизация (с настройкой гиперпараметров)\n",
    "6. построение модели\n",
    "7. оценка качества модели\n",
    "\n",
    "Обязательно использование векторайзеров:\n",
    "1. мешок n-грамм (диапазон для n подбирайте самостоятельно, запрещено использовать только униграммы).\n",
    "2. tf-idf ((диапазон для n подбирайте самостоятельно, также нужно подбирать гиперпараметры max_df, min_df, max_features)\n",
    "3. символьные n-граммы (диапазон для n подбирайте самостоятельно)\n",
    "\n",
    "В качестве классификатора нужно использовать наивный байесовский классификатор. \n",
    "\n",
    "Для сравнения векторайзеров между собой используйте precision, recall, f1-score и accuracy. Для этого сформируйте датафрейм, в котором в строках будут разные векторайзеры, а в столбцах разные метрики качества, а в  ячейках будут значения этих метрик для соответсвующих векторайзеров."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Bill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Bill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('russian')\n",
    "punctuation = string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "lemmatizer = MorphAnalyzer()\n",
    "\n",
    "def preprocessing(reviews, *, verbose=True):\n",
    "    i = 0\n",
    "    res = []\n",
    "    for review in reviews:\n",
    "        if verbose:\n",
    "            clear_output(wait=True)\n",
    "            print(f'{i}/{len(reviews)}, {(i / len(reviews) * 100):.2f}%')\n",
    "            i += 1\n",
    "            \n",
    "        review = review.lower()\n",
    "        for punct in punctuation:\n",
    "            review = review.replace(punct, ' ')\n",
    "        review = wt(review)\n",
    "        review = list(map(lambda x: lemmatizer.parse(x)[0].normal_form, review))\n",
    "        res.append(' '.join(review))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89999/90000, 100.00%\n"
     ]
    }
   ],
   "source": [
    "X = list(data.review)\n",
    "X = preprocessing(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data.sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save preprocessed data \n",
    "\n",
    "with open(\"preprocessed data\", \"wb\") as fp:\n",
    "    pickle.dump(X, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load preprocessed data\n",
    "\n",
    "with open(\"preprocessed data\", \"rb\") as fp:\n",
    "    X = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngram_range(min_n, max_n_from, max_n_to):\n",
    "    for i in range(min_n, max_n_from):\n",
    "        for j in range(i, max_n_to):\n",
    "            yield (i, j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "if os.name == 'nt':\n",
    "    proc = !echo %NUMBER_OF_PROCESSORS%\n",
    "    proc = proc[0]\n",
    "else:\n",
    "    proc = !nproc\n",
    "\n",
    "free_cores = 8\n",
    "proc = int(proc) - free_cores\n",
    "proc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CountVectorizer words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = make_pipeline(CountVectorizer(stop_words=stop_words, analyzer='word'),\n",
    "                      MultinomialNB())\n",
    "\n",
    "param_grid = {\n",
    "    'countvectorizer__ngram_range' : list(ngram_range(1, 5, 5))}\n",
    "\n",
    "search = GridSearchCV(model, refit=True, verbose=3, n_jobs=proc, scoring='f1_micro', param_grid=param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'countvectorizer__ngram_range': (1, 2)}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.fit(X_train, y_train)\n",
    "search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    neautral       0.61      0.63      0.62     10026\n",
      "    negative       0.71      0.66      0.68      9819\n",
      "    positive       0.82      0.86      0.84      9855\n",
      "\n",
      "    accuracy                           0.71     29700\n",
      "   macro avg       0.71      0.71      0.71     29700\n",
      "weighted avg       0.71      0.71      0.71     29700\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred = search.predict(X_test)\n",
    "report = classification_report(y_test, pred)\n",
    "words_rep = classification_report(y_test, pred, output_dict=True)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = make_pipeline(TfidfVectorizer(stop_words=stop_words, analyzer='word'),\n",
    "                      MultinomialNB())\n",
    "\n",
    "param_grid = {\n",
    "    'tfidfvectorizer__ngram_range' : list(ngram_range(1, 5, 5)),\n",
    "    'tfidfvectorizer__min_df' : np.linspace(0, 0.5, 4),\n",
    "    'tfidfvectorizer__max_df' : np.linspace(0.5, 1, 4)}\n",
    "    'tfidfvectorizer__max_features' '\n",
    "\n",
    "search = GridSearchCV(model, refit=True, verbose=3, n_jobs=proc, scoring='f1_micro', param_grid=param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 160 candidates, totalling 800 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bill\\anaconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:378: FitFailedWarning: \n",
      "520 fits failed out of a total of 800.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "520 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Bill\\anaconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Bill\\anaconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\pipeline.py\", line 378, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"C:\\Users\\Bill\\anaconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\pipeline.py\", line 336, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"C:\\Users\\Bill\\anaconda3\\envs\\pytorch\\lib\\site-packages\\joblib\\memory.py\", line 349, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"C:\\Users\\Bill\\anaconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\pipeline.py\", line 870, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"C:\\Users\\Bill\\anaconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 2079, in fit_transform\n",
      "    X = super().fit_transform(raw_documents)\n",
      "  File \"C:\\Users\\Bill\\anaconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1355, in fit_transform\n",
      "    X, self.stop_words_ = self._limit_features(\n",
      "  File \"C:\\Users\\Bill\\anaconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1187, in _limit_features\n",
      "    raise ValueError(\n",
      "ValueError: After pruning, no terms remain. Try a lower min_df or a higher max_df.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\Bill\\anaconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\model_selection\\_search.py:953: UserWarning: One or more of the test scores are non-finite: [0.69927032 0.71359867 0.71207297 0.71233831 0.67650083 0.67245439\n",
      " 0.67268657 0.5607131  0.5585738  0.42819237 0.40344942 0.40344942\n",
      " 0.40344942 0.40344942        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan 0.69927032 0.71359867\n",
      " 0.71207297 0.71233831 0.67650083 0.67245439 0.67268657 0.5607131\n",
      " 0.5585738  0.42819237 0.40344942 0.40344942 0.40344942 0.40344942\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan 0.69927032 0.71359867 0.71207297 0.71233831\n",
      " 0.67650083 0.67245439 0.67268657 0.5607131  0.5585738  0.42819237\n",
      " 0.40344942 0.40344942 0.40344942 0.40344942        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.69927032 0.71359867 0.71207297 0.71233831 0.67650083 0.67245439\n",
      " 0.67268657 0.5607131  0.5585738  0.42819237 0.40344942 0.40344942\n",
      " 0.40344942 0.40344942        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'tfidfvectorizer__max_df': 0.5,\n",
       " 'tfidfvectorizer__min_df': 0.0,\n",
       " 'tfidfvectorizer__ngram_range': (1, 2)}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.fit(X_train, y_train)\n",
    "search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    neautral       0.61      0.62      0.62     10026\n",
      "    negative       0.71      0.66      0.68      9819\n",
      "    positive       0.82      0.86      0.84      9855\n",
      "\n",
      "    accuracy                           0.71     29700\n",
      "   macro avg       0.71      0.71      0.71     29700\n",
      "weighted avg       0.71      0.71      0.71     29700\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred = search.predict(X_test)\n",
    "report = classification_report(y_test, pred)\n",
    "tfidf_rep = classification_report(y_test, pred, output_dict=True)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CountVectorizer char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = make_pipeline(CountVectorizer(stop_words=stop_words, analyzer='char'),\n",
    "                      MultinomialNB())\n",
    "\n",
    "param_grid = {\n",
    "    'countvectorizer__ngram_range' : list(ngram_range(1, 10, 10))}\n",
    "\n",
    "search = GridSearchCV(model, refit=True, verbose=3, n_jobs=proc, scoring='f1_micro', param_grid=param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 45 candidates, totalling 225 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'countvectorizer__ngram_range': (7, 9)}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.fit(X_train, y_train)\n",
    "search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    neautral       0.62      0.68      0.64     10026\n",
      "    negative       0.71      0.66      0.68      9819\n",
      "    positive       0.87      0.85      0.86      9855\n",
      "\n",
      "    accuracy                           0.73     29700\n",
      "   macro avg       0.73      0.73      0.73     29700\n",
      "weighted avg       0.73      0.73      0.73     29700\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred = search.predict(X_test)\n",
    "report = classification_report(y_test, pred)\n",
    "char_rep = classification_report(y_test, pred, output_dict=True)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CountVectorizer char</td>\n",
       "      <td>0.727374</td>\n",
       "      <td>0.732188</td>\n",
       "      <td>0.727374</td>\n",
       "      <td>0.728896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CountVectorizer words</td>\n",
       "      <td>0.714512</td>\n",
       "      <td>0.714356</td>\n",
       "      <td>0.714512</td>\n",
       "      <td>0.713850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TfidfVectorizer</td>\n",
       "      <td>0.712862</td>\n",
       "      <td>0.711455</td>\n",
       "      <td>0.712862</td>\n",
       "      <td>0.711574</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    name  accuracy  precision    recall        f1\n",
       "2   CountVectorizer char  0.727374   0.732188  0.727374  0.728896\n",
       "0  CountVectorizer words  0.714512   0.714356  0.714512  0.713850\n",
       "1        TfidfVectorizer  0.712862   0.711455  0.712862  0.711574"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reportdf = pd.DataFrame(columns=['accuracy', 'precision', 'recall', 'f1'])\n",
    "for report in [words_rep, tfidf_rep, char_rep]:\n",
    "    reportdf.loc[len(reportdf.index)] = [report['accuracy'], report['weighted avg']['precision'], report['weighted avg']['recall'], report['weighted avg']['f1-score']]\n",
    "\n",
    "reportdf['name'] = ['CountVectorizer words', 'TfidfVectorizer', 'CountVectorizer char']\n",
    "\n",
    "reportdf.insert(0, 'name', reportdf.pop('name'))\n",
    "reportdf.sort_values('f1', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5QYTwyMtWhAZ"
   },
   "source": [
    "## Задание 5.2 Регулярные выражения\n",
    "\n",
    "Регулярные выражения - способ поиска и анализа строк. Например, можно понять, какие даты в наборе строк представлены в формате DD/MM/YYYY, а какие - в других форматах. \n",
    "\n",
    "Или бывает, например, что перед работой с текстом, надо почистить его от своеобразного мусора: упоминаний пользователей, url и так далее.\n",
    "\n",
    "Навык полезный, давайте в нём тоже потренируемся.\n",
    "\n",
    "Для работы с регулярными выражениями есть библиотека **re**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "VaUW5S4gWhAb"
   },
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D6aYh7Osl8xr"
   },
   "source": [
    "В регулярных выражениях, кроме привычных символов-букв, есть специальные символы:\n",
    "* **?а** - ноль или один символ **а**\n",
    "* **+а** - один или более символов **а**\n",
    "* **\\*а** - ноль или более символов **а** (не путать с +)\n",
    "* **.** - любое количество любого символа\n",
    "\n",
    "Пример:\n",
    "Выражению \\*a?b. соответствуют последовательности a, ab, abc, aa, aac НО НЕ abb!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q7zOFFA3l_KQ"
   },
   "source": [
    "Рассмотрим подробно несколько наиболее полезных функций:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DbJrUpARWhAd"
   },
   "source": [
    "### findall\n",
    "возвращает список всех найденных непересекающихся совпадений.\n",
    "\n",
    "Регулярное выражение **ab+c.**: \n",
    "* **a** - просто символ **a**\n",
    "* **b+** - один или более символов **b**\n",
    "* **c** - просто символ **c**\n",
    "* **.** - любой символ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2athHzKuWhAd",
    "outputId": "ff2d7b25-31bb-4877-bd50-95442e6a8158"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abcd', 'abca']\n"
     ]
    }
   ],
   "source": [
    "result = re.findall('ab+c.', 'abcdefghijkabcabcxabc') \n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A9FpIw5RWhAf"
   },
   "source": [
    "Вопрос на внимательность: почему нет abcx?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B5ttzoxEWhAg"
   },
   "source": [
    "**Задание**: вернуть список первых двух букв каждого слова в строке, состоящей из нескольких слов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'качество плохое пошив ужасный (горловина наперекос) Фото не соответствует Ткань ужасная рисунок блеклый маленький рукав не такой УЖАС!!!!! не стоит за такие деньги г.......'"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = data.review[0]\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "id": "7ZR2AEq3WhAg"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ка',\n",
       " 'пл',\n",
       " 'по',\n",
       " 'уж',\n",
       " 'го',\n",
       " 'на',\n",
       " 'Фо',\n",
       " 'не',\n",
       " 'со',\n",
       " 'Тк',\n",
       " 'уж',\n",
       " 'ри',\n",
       " 'бл',\n",
       " 'ма',\n",
       " 'ру',\n",
       " 'не',\n",
       " 'та',\n",
       " 'УЖ',\n",
       " 'не',\n",
       " 'ст',\n",
       " 'за',\n",
       " 'та',\n",
       " 'де']"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(r\"\\b\\w{2}\", text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MI18l-l9WhAk"
   },
   "source": [
    "### split\n",
    "разделяет строку по заданному шаблону\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sVKdRoc1WhAl",
    "outputId": "78b5d289-8e8c-4621-fe3c-34aa130c727c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['itsy', ' bitsy', ' teenie', ' weenie']\n"
     ]
    }
   ],
   "source": [
    "result = re.split(',', 'itsy, bitsy, teenie, weenie') \n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "10u5efuSWhAm"
   },
   "source": [
    "можно указать максимальное количество разбиений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9U9EQZMwWhAn",
    "outputId": "45699604-9950-4185-cd4f-0e3ad6655629"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['itsy', ' bitsy', ' teenie, weenie']\n"
     ]
    }
   ],
   "source": [
    "result = re.split(',', 'itsy, bitsy, teenie, weenie', maxsplit=2) \n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0EMcMyflWhAp"
   },
   "source": [
    "**Задание**: разбейте строку, состоящую из нескольких предложений, по точкам, но не более чем на 3 предложения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "id": "dVgPSjEOWhAp"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Какие-то странные предложения. Зачем. Ставить. Столько. Точек. Никто не знает'"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'Какие-то странные предложения. Зачем. Ставить. Столько. Точек. Никто не знает'\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Какие-то странные предложения',\n",
       " ' Зачем',\n",
       " ' Ставить. Столько. Точек. Никто не знает']"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.split(r\"\\.\", text, maxsplit=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1wrEGqBSWhAr"
   },
   "source": [
    "### sub\n",
    "ищет шаблон в строке и заменяет все совпадения на указанную подстроку\n",
    "\n",
    "параметры: (pattern, repl, string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "az3KxKWwWhAr",
    "outputId": "93f260be-ce79-405c-b21a-5ca8345ff4b7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bbcbbc\n"
     ]
    }
   ],
   "source": [
    "result = re.sub('a', 'b', 'abcabc')\n",
    "print (result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qD0n7_HPWhAt"
   },
   "source": [
    "**Задание**: напишите регулярное выражение, которое позволит заменить все цифры в строке на \"DIG\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "id": "s_Sdu7xlWhAu"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Хотелось бы скрыть номер карты в этом предложении 4485 5647 8756 7909'"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'Хотелось бы скрыть номер карты в этом предложении 4485 5647 8756 7909'\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Хотелось бы скрыть номер карты в этом предложении DIGDIGDIGDIG DIGDIGDIGDIG DIGDIGDIGDIG DIGDIGDIGDIG'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub(r'\\d', 'DIG', text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b8__oi1PWhAv"
   },
   "source": [
    "**Задание**: напишите  регулярное выражение, которое позволит убрать url из строки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'И зачем здесь ссылка на википедию https://ru.wikipedia.org/wiki/%D0%9C%D0%B0%D0%B3%D0%BC%D0%B0_(%D0%B0%D0%BB%D0%B3%D0%B5%D0%B1%D1%80%D0%B0) ?'"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'И зачем здесь ссылка на википедию https://ru.wikipedia.org/wiki/%D0%9C%D0%B0%D0%B3%D0%BC%D0%B0_(%D0%B0%D0%BB%D0%B3%D0%B5%D0%B1%D1%80%D0%B0) ?'\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "id": "KwNS9zt4WhAv"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'И зачем здесь ссылка на википедию  ?'"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub(r'http[s]:\\/\\/[\\w./%\\d(){}]*', '', text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gStgBJy2WhAx"
   },
   "source": [
    "### compile\n",
    "компилирует регулярное выражение в отдельный объект"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JstTupisWhAy",
    "outputId": "f60e13a6-9d93-46e5-f329-620f6072715f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Слова', 'Да', 'больше', 'ещё', 'больше', 'слов', 'Что-то', 'ещё']"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Пример: построение списка всех слов строки:\n",
    "prog = re.compile('[А-Яа-яё\\-]+')\n",
    "prog.findall(\"Слова? Да, больше, ещё больше слов! Что-то ещё.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WXEXc3G0WhA2"
   },
   "source": [
    "**Задание**: для выбранной строки постройте список слов, которые длиннее трех символов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "id": "nFvnIWbUWhA2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Магма (группоид) в общей алгебре — алгебра, состоящая из множества М с одной бинарной операцией M × M → M. Помимо требования замкнутости множества относительно заданной на нём операции, других требований к операции и множеству не предъявляется.'"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'Магма (группоид) в общей алгебре — алгебра, состоящая из множества М с одной бинарной операцией M × M → M. Помимо требования замкнутости множества относительно заданной на нём операции, других требований к операции и множеству не предъявляется.'\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Магма',\n",
       " 'группоид',\n",
       " 'общей',\n",
       " 'алгебре',\n",
       " 'алгебра',\n",
       " 'состоящая',\n",
       " 'множества',\n",
       " 'одной',\n",
       " 'бинарной',\n",
       " 'операцией',\n",
       " 'Помимо',\n",
       " 'требования',\n",
       " 'замкнутости',\n",
       " 'множества',\n",
       " 'относительно',\n",
       " 'заданной',\n",
       " 'операции',\n",
       " 'других',\n",
       " 'требований',\n",
       " 'операции',\n",
       " 'множеству',\n",
       " 'предъявляется']"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regexp = re.compile(r'\\w{4,}')\n",
    "regexp.findall(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SQDNZ3HQWhA3"
   },
   "source": [
    "**Задание**: вернуть список доменов (@gmail.com) из списка адресов электронной почты:\n",
    "\n",
    "```\n",
    "abc.test@gmail.com, xyz@test.in, test.first@analyticsvidhya.com, first.test@rest.biz\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'abc.test@gmail.com, xyz@test.in, test.first@analyticsvidhya.com, first.test@rest.biz'"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'abc.test@gmail.com, xyz@test.in, test.first@analyticsvidhya.com, first.test@rest.biz'\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['@gmail.com', '@test.in', '@analyticsvidhya.com', '@rest.biz']"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regexp = re.compile(r'@\\w*.\\w*')\n",
    "regexp.findall(text)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "Zpq4QOU5Wg-H",
    "i_7DyyXRWg-K",
    "_JewKs4XU-so",
    "5yiLk1P_xYQ2",
    "VlWxW3e9Wg-m",
    "D39SSh0zWg-r",
    "rhVrgkSaWg_K",
    "XsRf9T_SWg_U",
    "ylKZG2MwWg_f",
    "9hedBdcYWhAH",
    "JrqW55jgWhAR",
    "5QYTwyMtWhAZ",
    "DbJrUpARWhAd",
    "MI18l-l9WhAk",
    "1wrEGqBSWhAr",
    "gStgBJy2WhAx"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
